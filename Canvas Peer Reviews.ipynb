{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Peer Review Info\n",
    "\n",
    "## Summary\n",
    "*Peer Review Info* is a Jupyter Notebook and Python script that interacts with the Canvas LMS to create formatted .csv tables containing peer review data for an assignment. Upon providing the necessary inputs, the notebook will produce two .csv files in the \"peer_review_data\" folder (in project root directory). The data tables will give users an overview all assigned peer reviews for a given assignment - including all students who've been assigned as assessors, who they are assessing and the results of any completed assessments.\n",
    "\n",
    "### To Run (once you have read all of the instructions):\n",
    "**Kernel -> Restart & Run All**\n",
    "\n",
    "---\n",
    "\n",
    "## Input\n",
    "* Base URL *(Instance of Canvas being used - ex. https://ubc.instructure.com)*\n",
    "* Canvas Token *(generate through Account => Settings)*\n",
    "* Course ID *(last digits of URL when visiting course page)*\n",
    "* Assignment ID *(last digits of URL when visiting assignment page)*\n",
    "* To include the assignment scores if graded in addition to peer reviewed (generates additional csv) *(y/n)*\n",
    "\n",
    "## Output\n",
    "\n",
    "### peer_review_assessments.csv:\n",
    "*Lists all assigned assessments including roles of assessee/assessor, total score and score given for each rubric item (Note: all columns pertaining to score will be blank if a review is not completed yet).*\n",
    "\n",
    "\n",
    "* **Assessee:** Name of the student who's work is being evaluated.\n",
    "* **Assessor:** Name of the student who is evaluating the assessee.\n",
    "* **Total Score (```points_possible```):** The total score given by the assessor to the assessee, where ```points possible``` is the maximum possible score for the assignment.\n",
    "* **```criteria_description``` (```criteria_points```):** The score breakdown per criteria item as they appear in the rubric. Will be as many columns as criteria items in rubric **(1...n)**. ```criteria_description``` will be the the heading of a single rubric item and ```criteria_points``` is the maximum possible score for that item.\n",
    "    \n",
    "    \n",
    "### peer_review_overview.csv:\n",
    "*Lists each student in the course by canvas user id and name, shows # of assigned peer reviews as well as # of completed reviews; for each student, if that student has been evaluated, their scores will appear in the \"Review\" columns.*\n",
    "\n",
    "* **Canvas User ID:** The user id of the student as it appears on Canvas.\n",
    "* **Name:** The student's name\n",
    "* **Num Assigned Peer Reviews:** The number of peer reviews that have been assigned to the student.\n",
    "* **Num Completed Peer Reviews:** The number of peer reviews that have been completed by the student.\n",
    "* **Review: ```review_number```:** The score the student has been awarded from a single peer review (blank if review is not complete). Will be as many columns as there are completed peer reviews for a particular student **(1...n)** ```review_number``` will count up from 1 to help identify one review from another.\n",
    "\n",
    "### peer_review_given_score.csv\n",
    "*(optional) Lists each student in the course by canvas user id, shows the non-peer-review given score (if graded in addition to peer reviewed). This is an optional output.*\n",
    "\n",
    "- **CanvasUserID:** The user id of the student as it appears on Canvas.\n",
    "- **GradebookScore:** The score found in the gradebook.\n",
    "- **GradingWorkflowState:** Details about the grading workflow state. \n",
    "\n",
    "---\n",
    "\n",
    "*authors: @markoprodanovic @alisonmyers*\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. INITIALIZATION\n",
    "Run the following block of code to initialize python modules."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "sys.path.insert(0, os.path.abspath('./src'));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 2. GET PEER REVIEW DATA\n",
    "Run the block of code below. Input token, canvas instance, course number and assignment number when prompted.\n",
    "\n",
    "\n",
    "#### Canvas Instance URLS *(copy and paste from list when prompted):*\n",
    "* https://ubc.instructure.com\n",
    "* https://ubc.test.instructure.com"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from peer_review import main\n",
    "main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
