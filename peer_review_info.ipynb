{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Peer Review Info\n",
    "\n",
    "### Summary\n",
    "*Peer Review Info* is a Jupyter Notebook and Python script that works with Canvas LMS Data to create formatted .csv tables containing Canvas peer review data. Upon providing the necessary inputs, the notebook will produce three .csv files in /data in the project root directory. The data tables will give users an overview all assigned peer reviews for a given assignment - including all students who've been assigned as assessors, who they are assessing and the results of any completed assessments.\n",
    "\n",
    "---\n",
    "\n",
    "### Input\n",
    "* Canvas token *(generate through Account => Settings)*\n",
    "* Course ID *(last digits of URL when visiting course page)*\n",
    "* Assignment ID *(last digits of URL when visiting assignment page)*\n",
    "* Base URL *(Instance of Canvas being used - default: canvas.ubc.ca)*\n",
    "\n",
    "### Output\n",
    "* **users.csv:**  Complete list of users in specified course and number of assigned/completed peer reviews\n",
    "* **peer_reviews.csv:**  Specifies assessor/assessee pairings and scores (for completed assessments)\n",
    "* **items.csv:**  Completed assessments indentified by a unique id and scores for each item in rubric\n",
    "* **merged.csv:**  Three tables above merged:\n",
    "\n",
    "    * users.csv[**CanvasUserID**] <=> peer_reviews.csv[**AssessorID**]\n",
    "\n",
    "    * peer_reviews.csv[**EvalID**] <=> items.csv[**EvalID**]\n",
    "\n",
    "---\n",
    "\n",
    "*authors: @alisonmyers @markoprodanovic*\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## IMPORT STATEMENTS\n",
    "Do not remove the following import statements. They are required for the notebook/script to run successfully."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ipywidgets as widgets\n",
    "from peer_review_info import peer_review"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 1. USER INPUTS\n",
    "Run the following block of code. Prompts will appear where tokens, course ids, assignment ids and canvas instance can be specified."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "token = input('Canvas Token: ')\n",
    "course = input('Course ID: ')\n",
    "asmt_id = input('Assignment ID: ')\n",
    "\n",
    "\n",
    "CANVAS_INSTANCES = ['https://canvas.ubc.ca',\n",
    "                    'https://ubc.instructure.com',\n",
    "                    'https://ubc.test.instructure.com',\n",
    "                    'https://ubcsandbox.instructure.com']\n",
    "\n",
    "url_selector = widgets.Dropdown(\n",
    "                options=CANVAS_INSTANCES,\n",
    "                value='https://canvas.ubc.ca',\n",
    "                description='Instance:',\n",
    "                disabled=False,\n",
    "            )\n",
    "\n",
    "display(url_selector)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 2. INPUT CONFIRMATION\n",
    "Run the following block of code to see entered inputs. Confirm they are correct."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = {\n",
    "    'token': token,\n",
    "    'url': url_selector.value,\n",
    "    'course': course,\n",
    "    'assignment_id': asmt_id\n",
    "}\n",
    "\n",
    "print('Token: ' + inputs['token'])\n",
    "print('Base URL: ' + inputs['url'])\n",
    "print('Course ID: ' + inputs['course'])\n",
    "print('Assignment ID: ' + inputs['assignment_id'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 3. Run\n",
    "Once user inputs have been confirmed, run the following block of code to execute the script and produce output tables.\n",
    "Any errors that occur during execution will display below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "peer_review(inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
